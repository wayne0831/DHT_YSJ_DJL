pkgs <- c("rpart",  "crayon")

#sapply(pkgs, install.packages, character.only = TRUE)

sapply(pkgs, require, character.only = TRUE)

####################################################################################################
## k > 1  
## red   : more weight on mis-classified data 
## green : basic model
## blue  : more weight on well-classified data 

# -1 : k < 1 ; more weight on well classified data
# 0 : k = 1 ; basic model
# 1 : k > 1 ; more weight on misclassified data

####################################################################################################
# set work directory
setwd("C:\\Users\\Administrator\\Desktop\\Dataset")

# load the dataset
data          <- read.csv('sonar.csv', TRUE)

# heuristic parameter which controls the value of alpha
k             <<- 2

# seed
s             <- 3

# feature location of idependent and dependent variables
independent   <- 1:60
dependent     <- 62

# number of iterations of the model
iterations    <- 100

# store the result of each model(heuristic, basic)
result_h      <- list()
result_b      <- list()

# divide the dataset into input features and output feature
X             <- data[, independent]
y             <- data[, dependent]

####################################################################################################
# predict function
predict.adaboost <- function(object, X, type = c("response", "prob"),
                             n_tree = NULL, ...){
  # handle args
  type <- match.arg(type)
  
  if(is.null(n_tree)){
    
    tree_seq <- seq_along(object$alphas)
    
  } else{
    
    if(n_tree > length(object$alpha))
      
      stop('n_tree must be less than the number of trees used in fit')
    
    tree_seq <- seq(1, n_tree)
    
  }
  
  # evaluate score function on sample
  f <- 0
  
  for(i in tree_seq){
    
    tree       <- object$trees[[i]]
    
    tree$terms <- object$terms
    
    pred       <- as.integer(as.character(stats::predict(tree, data.frame(X),
                                                         type = "class")))
    f          <- f + object$alphas[i] * pred
    
  }
  
  # handle response type
  if(type == "response"){
    
    sign(f)
    
  } else if(type == "prob"){
    
    1/(1 + exp(-2 * f))
    
  }
  
}

# basic adaboost function
# our base model is decision stump, which splits only once
basic_adaboost <- function(X, y, n_rounds = 100, 
                           control = rpart.control(cp = -1, maxdepth = 1)){
  
  # count the number of rows
  n      <- nrow(X)  
  
  # initialize weight on each data 
  w      <- rep(1/n, n)
  
  # initalize tree and alpha
  trees  <- list()
  alphas <- list()
  
  # build weak classifiers
  for(i in seq(n_rounds)){
    
    tree <- rpart::rpart(y ~ ., 
                         data = data.frame(X), weights = w,
                         method = "class", control = control, 
                         x = FALSE, y = FALSE, model = TRUE)
    
    
    pred <- as.integer(as.character(stats::predict(tree, data.frame(X), type = "class")))
    
    # calculate the error of each classifiers
    e    <- sum(w * (pred != y))
    
    # if error >= 0.5, flip the result
    if(e >= 0.5){
      
      e <- 1 - e
      
    }
    
    else{
      
      e <- e
      
    }
    # learning rate(weight) of each classifiers
    alpha <- 1/2 * log((1-e)/e)
    
    # update weight of each data
    w     <- w * exp(-alpha*pred*y)
    
    # normalize weight of each data
    w     <- w / sum(w)
    
    # If classifier's error rate is nearly 0, boosting process ends
    if(abs(e) < 1e-5){
      # if first base classifier predicts data perfectly, boosting process ends
      if(i == 1){
        # first base classifier's weight should be 1
        alphas[[i]] <- 1
        
        trees[[i]]  <- tree
        
        terms       <- tree$terms
        
        break
        
      }
      
      break
      
    }
    
    
    
    # Remove formulas since they waste memory
    if(i == 1){
      
      terms       <- tree$terms
      
    }
    
    else{
      
      tree$terms <- NULL
      
    }
    
    alphas[[i]] <- alpha
    
    trees[[i]]  <- tree
    
  }
  
  
  
  
  
  out        <- list(alphas = unlist(alphas), 
                     trees = trees, 
                     terms = terms)
  
  class(out) <- "adaboost"
  
  # create confusion matrix for in-sample fits
  y_hat                <- stats::predict(out, X)
  
  out$confusion_matrix <- table(y, y_hat)
  
  out
  
}

heuristic_adaboost <- function(X, y, n_rounds = 100, 
                               control = rpart.control(cp = -1, maxdepth = 1)){
  
  # count the number of rows
  n      <- nrow(X)  
  
  # initialize weight on each data 
  w      <- rep(1/n, n)
  
  # initalize tree and alpha
  trees  <- list()
  alphas <- list()
  
  
  # build weak classifiers
  for(i in seq(n_rounds)){
    tree <- rpart::rpart(y ~ ., 
                         data = data.frame(X), weights = w,
                         method = "class", control = control, 
                         x = FALSE, y = FALSE, model = TRUE)
    
    pred <- as.integer(as.character(stats::predict(tree, data.frame(X), type = "class")))
    
    # calculate the error of each classifiers
    e    <- sum(w * (pred != y))
    
    # if error >= 0.5, flip the result
    if(e >= 0.5){
      
      e <- 1 - e
      
    }
    
    else{
      
      e <- e
      
    }
    
    # if k > 1 model is best
    if (heu_list[i] == 1){ #more weight in mis
      
      #alpha <- (1/(h + 1)) * log((1/h) * ((1-e)/e))
      alpha <- 1/(2*k) * log((1-e)/e)
      
      # update weight of each data
      for (tt in 1:length(w)){
        # if prediction == actual value , less weight on data
        if (alpha * pred[tt] * y[tt] > 0){
          
          w[tt] <- w[tt] * (exp(k * (-alpha)))
          
        }
        # if prediction != actual value, more weight on data
        else{
          
          w[tt] <- w[tt] * (exp(k * (alpha)))
          
        }
        
      }
      
    }
    
    # if basic model is best
    if (heu_list[i] == 0){
      
      alpha <- 1/2 * log((1-e)/e)
      
      # update weight of each data
      w     <- w * exp(-alpha * pred * y)
      
    }
    
    # if k < 1 model is best
    if (heu_list[i] == -1){
      
      #alpha <- (h+1)) * log((h)*((1-e)/e))
      alpha <- (k/2) * log((1-e)/e)
      
      # update weight of each data
      for (tt in 1:length(w)){
        # if prediction == actual value , less weight on data
        if (alpha * pred[tt] * y[tt] > 0){
          
          w[tt] <- w[tt] * (exp((1/k)*(-alpha)))
          
        }
        # if prediction != actual value, more weight on data
        else{
          
          w[tt] <- w[tt] * (exp((1/k)*(alpha)))
          
        }
        
      }
      
    }
    
    # normalize weight of each data
    w     <- w / sum(w)
    
    # If classifier's error rate is nearly 0, boosting process ends
    if(abs(e) < 1e-5){
      
      # if first base classifier predicts data perfectly, boosting process ends
      if(i == 1){
        
        # first base classifier's weight should be 1
        alphas[[i]] <- 1
        
        trees[[i]]  <- tree
        
        terms       <- tree$terms
        
        break
        
      }
      
      break
      
    }
    
    
    
    # Remove formulas since they waste memory
    if(i == 1){
      
      terms       <- tree$terms
      
    } 
    
    else{
      
      tree$terms <- NULL
      
    }
    
    trees[[i]]  <- tree
    
    alphas[[i]] <- alpha
    
  }
  
  out        <- list(alphas = unlist(alphas), 
                     trees = trees, 
                     terms = terms)
  
  class(out) <- "adaboost"
  
  # create confusion matrix for in-sample fits
  y_hat                <- stats::predict(out, X)
  
  out$confusion_matrix <- table(y, y_hat)
  
  out
  
}


# basic adaboost k-fold cross validation
# tt : number of rounds
kfold_basic <- function(tt){
  # number of cross validation
  k       <- 5
  
  # fix the seed so that each model train and test the same data
  set.seed(s) 
  
  # each data has its own id(1 to 5) to process k fold cross validation 
  data$id <- sample(1:k, nrow(data), replace = TRUE)
  
  list    <- 1:k
  
  # data frame reset
  prediction_bm <- testset_copy_bm <- data.frame()
  
  #function for k fold
  for(i in 1:k){
    
    # remove rows with id i from dataframe to create training set
    # select rows with id i to create test set
    trainset     <- subset(data, id %in% list[-i])
    testset      <- subset(data, id %in% c(i))
    
    #run a adaboost model
    model_bm        <- basic_adaboost(trainset[, independent], trainset[, dependent], n_rounds = tt)
    
    temp_bm         <- as.data.frame(predict(model_bm, testset))
    
    # append this iteration's prediction to the end of the prediction data frame
    prediction_bm   <- rbind(prediction_bm, temp_bm)
    
    # append this iteration's test set to the testset copy data frame
    testset_copy_bm <- rbind(testset_copy_bm, as.data.frame(testset[, dependent]))
    
    
    # add predictions and actual Sepal Length values
    result_bm            <- cbind(prediction_bm, testset_copy_bm[, 1])
    
    names(result_bm)     <- c("Actual", "Predicted")
    
    confusion_matrix_bm  <- table(result_bm$Actual, result_bm$Predicted )
    
    accuracy_bm          <- sum(diag(confusion_matrix_bm)) / sum(confusion_matrix_bm)
    
    result_bm            <- list("confusion_matrix_bm " = confusion_matrix_bm, 
                                 "accuracy_bm"          = accuracy_bm)
    
  }  
  
  temp1 <<- (result_bm$accuracy_bm)
  
  return(temp1)
}

# heuristic adaboost k-fold cross validation
# tt : number of rounds
kfold_heu <- function(tt){
  # number of cross validation
  k       <- 5
  
  # fix the seed so that each model train and test the same data
  set.seed(s) 
  
  # each data has its own id(1 to 5) to process k fold cross validation 
  data$id <- sample(1:k, nrow(data), replace = TRUE)
  
  list    <- 1:k
  
  # data frame reset
  prediction_hm <- testset_copy_hm <- data.frame()
  
  #function for k fold
  for(i in 1:k){
    # remove rows with id i from dataframe to create training set
    # select rows with id i to create test set
    trainset     <- subset(data, id %in% list[-i])
    testset      <- subset(data, id %in% c(i))
    
    #run a adaboost model
    model_hm        <- heuristic_adaboost(trainset[, independent], trainset[, dependent], n_rounds = tt)  
    
    temp_hm         <- as.data.frame(predict(model_hm, testset))
    
    # append this iteration's prediction to the end of the prediction data frame
    prediction_hm   <- rbind(prediction_hm, temp_hm)
    
    # append this iteration's test set to the testset copy data frame
    testset_copy_hm <- rbind(testset_copy_hm, as.data.frame(testset[, dependent]))
    
    # add predictions and actual Sepal Length values
    result_hm            <- cbind(prediction_hm, testset_copy_hm[, 1])
    
    names(result_hm)     <- c("Actual", "Predicted")
    
    confusion_matrix_hm  <- table(result_hm$Actual, result_hm$Predicted )
    
    accuracy_hm          <- sum(diag(confusion_matrix_hm)) / sum(confusion_matrix_hm)
    
    result_hm            <- list("confusion_matrix_hm " = confusion_matrix_hm, 
                                 "accuracy_hm"          = accuracy_hm)
    
  }  
  
  temp2 <<- (result_hm$accuracy_hm)
  
  return (temp2)
  
}


plot_result <- function(qwe){
  # initialize heu_list and res_3
  heu_list <<- list()
  res_3    <- list()
  
  for (i1 in 1: qwe){
    # run baisc adaboost k-fold cross validation
    kfold_basic(i1)
    
    # append the baisc adaboost k-fold cross validation result
    result_b <<- append(result_b, temp1, after = length(result_b))
    
    # color of accuracy is blue
    cat(blue(temp1)) 
    cat(("\n"))
    
    for (i2 in -1:1){
      #res_3 <- append(res_3, adaboost(append(heu_list,i2)), after = length(res_3))
      heu_list <<- append(heu_list,i2)
      res_3    <- append(res_3, kfold_heu(i1), after = length(res_3))
      heu_list <<- heu_list[1:length(heu_list)-1]
      
    }
    
    result_h <<- append(result_h, max(unlist(res_3)), after = length(result_h))
    cat(red(max(unlist(res_3))))
    cat(("\n"))
    
    # if accuracy of 3 models are same, store 0(basic model)
    if (max(unlist(res_3)) == res_3[1]){
      
      if(max(unlist(res_3)) == res_3[2]){
        
        if(max(unlist(res_3)) == res_3[3]){
          heu_list <<- append(heu_list, 0, after = length(heu_list))
        }
        
        else{
          heu_list <<- append(heu_list, 0, after = length(heu_list))
        }
      }
      
      else if(max(unlist(res_3)) == res_3[3]){
        heu_list <<- append(heu_list, -1, after = length(heu_list))
      }
      
      else{
        heu_list <<- append(heu_list, -1, after = length(heu_list))
      }
      
    }
    
    else if (max(unlist(res_3)) == res_3[2]){
      
      if(max(unlist(res_3)) == res_3[3]){
        heu_list <<- append(heu_list, 0, after = length(heu_list))
      }
      
      else{
        heu_list <<- append(heu_list, 0, after = length(heu_list))
      }
      
    }
    
    else{
      heu_list <<- append(heu_list, 1, after = length(heu_list))
    }
    
    res_3 <- list()
    
  }  
  # show accuracy of heuristic adaboost
  cat(green("#################################"))
  cat(("\n"))
  cat(green("maximum heuristic accuracy"))
  cat(("\n"))
  cat(red(max(unlist(result_h))))
  cat(("\n"))
  cat(green("mean heuristic accuracy"))
  cat(("\n"))
  cat(red(mean(unlist(result_h))))
  cat(("\n"))
  cat(green("minimum heuristic accuracy"))
  cat(("\n"))
  cat(red(min(unlist(result_h))))
  cat(("\n"))
  cat(green("variance of heuristic accuracy"))
  cat(("\n"))
  cat(red(var(unlist(result_h))))
  cat(("\n"))
  
  # show accuracy of basic adaboost
  cat(green("#################################"))
  cat(("\n"))
  cat(green("maximum basic accuracy"))
  cat(("\n"))
  cat(blue(max(unlist(result_b))))
  cat(("\n"))
  cat(green("mean basic accuracy"))
  cat(("\n"))
  cat(blue(mean(unlist(result_b))))
  cat(("\n"))
  cat(green("minimum basic accuracy"))
  cat(("\n"))
  cat(blue(min(unlist(result_b))))
  cat(("\n"))
  cat(green("variance of basic accuracy "))
  cat(("\n"))
  cat(blue(var(unlist(result_b))))
  cat(("\n"))
  
  # plot the graph of heuristic adaboost
  par(bg = "gray")
  plot(1:iterations, result_h, col = "red", ylim = c(min(min(unlist(result_b)), 
                                                         min(unlist(result_h))), 
                                                     max(max(unlist(result_b)), 
                                                         max(unlist(result_h)))))
  
  lines(1:iterations,result_h,col = "red")
  
  # plot the graph of basic adaboost
  par(new = TRUE)
  plot(1:iterations, result_b, col = "blue", xlab = "", ylab = "", 
       ylim = c(min(min(unlist(result_b)), 
                    min(unlist(result_h))), 
                max(max(unlist(result_b)), 
                    max(unlist(result_h)))))
  lines(1:iterations, result_b, col = "blue")
  
}


system.time(plot_result(iterations))





