
    set.seed(1)
sample(1:10)

pkgs <- c("rpart", "crayon", "caTools", "ada", "caret")

sapply(pkgs, require, character.only = TRUE)

setwd("C:\\Users\\Wayne\\Desktop\\Dataset_Final")

data          <- read.csv('Sonar.csv', TRUE)

independent   <- 1:60
dependent     <- 61

# divide the dataset into input features and output feature
X             <- data[, independent]
y             <- data[, dependent]

####################################################################################################
# predict function
####################################################################################################
predict.adaboost <- function(object, X, type = c("response", "prob"), n_tree = NULL){
  # handle args
  type <- match.arg(type)
  
  if(is.null(n_tree)) { tree_seq <- seq_along(object$alphas) } 
  
  else                { tree_seq <- seq(1, n_tree) }
  
  # evaluate score function on sample
  f <- 0
  
  for(i in tree_seq){
    tree       <- object$trees[[i]]
    tree$terms <- object$terms
    pred       <- as.integer(as.character(stats::predict(tree, data.frame(X), type = "class")))
    f          <- f + object$alphas[i] * pred
  }
  
  # handle response type
  if(type == "response")  { sign(f) } 
  else if(type == "prob") { 1/(1 + exp(-2 * f)) }
}

basic.adaboost <- function(X, y, n_rounds = 100,
                           control = rpart.control(cp = -1, maxdepth = 1)){
  # count the number of rows
  n      <- nrow(X)
  
  # initialize weight on each data, tree and alpha
  w      <- list(rep(1/n, n))
  trees  <- list()
  alphas <- list()
  
  w.list <- list()
  w.temp <- list()
  res.list <<- list()
  alphas.list <<- list()
  tree.list <<- list()
  
  # build weak classifiers
  for(i in seq(n_rounds)){
    
    tree <- rpart::rpart(y ~ .,
                         data = data.frame(X), weights = w[[i]],
                         method = "class", control = control,
                         x = FALSE, y = FALSE, model = TRUE)
    
    pred <- as.integer(as.character(stats::predict(tree, data.frame(X), type = "class")))
    
    # calculate the error of each classifiers
    e    <- sum(w[[i]] * (pred != y))
    
    # if error >= 0.5, flip the result
    if(e >= 0.5) { e <- 1 - e }
    
    # learning rate(weight) of each classifiers
    n_count = 0
    for(i1 in k.list){
      n_count = n_count + 1
      #alpha <- (1/(2*k.path[[i]])) * log((1-e)/e)
      #alpha <- 1/2 * log((1-e)/e)
      
      alpha <- 1/(2*i1) * log((1-e)/e)
      #print(2*i1)
      
      # update and normalize weight of each data
      w.list[[n_count]]     <- w[[i]] * exp(-alpha*pred*y)
      w.list[[n_count]]     <- w.list[[n_count]] / sum(w.list[[n_count]])
      
      # If classifier's error rate is nearly 0, boosting process ends
      if(abs(e) < 1e-5){
        # if first base classifier predicts data perfectly, boosting process ends
        if(i == 1){
          # first base classifier's weight should be 1
          alphas[[i]] <- 1
          trees[[i]]  <- tree
          terms       <- tree$terms
        }
      }
      
      # Remove formulas since they waste memory
      if(i == 1)  { terms       <- tree$terms }
      else        { tree$terms  <- NULL }
      
      alphas[[i]] <- alpha
      trees[[i]]  <- tree
      
      alphas.list[[n_count]] <- alpha
      tree.list[[n_count]]   <- tree
      
      result        <- list(terms  = terms,
                            trees  = trees,
                            alphas = unlist(alphas))
      
      class(result) <- "adaboost"
      
      y_hat                   <- stats::predict(result, X)
      result$confusion_matrix <- table(y, y_hat)
      
      res.list <<- append(res.list, (sum(diag(result$confusion_matrix)) / sum(result$confusion_matrix)), after = length(res.list))
     
      alphas[[i]] <- NULL
      trees[[i]] <- NULL
    }

    if (res.list[[1]] > res.list[[2]] & res.list[[1]] > res.list[[3]]){
      k.path      <<- append(k.path, 1/k.k, after = length(k.path))
      w[[i+1]]    <- w.list[[1]]
      alphas[[i]] <- alphas.list[[1]]
      trees[[i]]  <- tree.list[[1]]
    }
    else if (res.list[[2]] > res.list[[1]] & res.list[[2]] > res.list[[3]]){
      k.path      <<- append(k.path, 1, after = length(k.path))
      w[[i+1]]    <- w.list[[2]]
      alphas[[i]] <- alphas.list[[2]]
      trees[[i]]  <- tree.list[[2]]
    }
    
    else if (res.list[[3]] > res.list[[1]] & res.list[[3]] > res.list[[2]]){
      k.path      <<- append(k.path, k.k, after = length(k.path))
      w[[i+1]]    <- w.list[[3]]
      alphas[[i]] <- alphas.list[[3]]
      trees[[i]]  <- tree.list[[3]]
    }
    
    else if (res.list[[1]] > res.list[[3]] & res.list[[1]] == res.list[[2]]){
      k.path      <<- append(k.path, 1, after = length(k.path))
      w[[i+1]]    <- w.list[[2]]
      alphas[[i]] <- alphas.list[[2]]
      trees[[i]]  <- tree.list[[2]]
    }
    
    else if (res.list[[1]] > res.list[[2]] & res.list[[1]] == res.list[[3]]){
      k.path      <<- append(k.path, k.k, after = length(k.path))
      w[[i+1]]    <- w.list[[3]]
      alphas[[i]] <- alphas.list[[3]]
      trees[[i]]  <- tree.list[[3]]
    }
    
    else if (res.list[[2]] > res.list[[1]] & res.list[[2]] == res.list[[3]]){
      k.path      <<- append(k.path, 1, after = length(k.path))
      w[[i+1]]    <- w.list[[2]]
      alphas[[i]] <- alphas.list[[2]]
      trees[[i]]  <- tree.list[[2]]
    }
    
    else{
      k.path      <<- append(k.path, 1, after = length(k.path))
      w[[i+1]]    <- w.list[[2]]
      alphas[[i]] <- alphas.list[[2]]
      trees[[i]]  <- tree.list[[2]]
    }
    
    #print(res.list[which.max(res.list)])
    res.list   <<- list()
    w.list     <<- list()
    alpha.list <<- list()
    tree.list  <<- list()
    #print(alphas[[i]])
  }
  
  result        <- list(terms  = terms,
                        trees  = trees,
                        alphas = unlist(alphas))
  
  class(result) <- "adaboost"
  
  y_hat                   <- stats::predict(result, X)
  result$confusion_matrix <- table(y, y_hat)
  #print((sum(diag(result$confusion_matrix)) / sum(result$confusion_matrix)))
  #print(result$alphas)
  return(result)
}

kfold.ada <- function(iteration){
  # number of cross validation
  k       <- 5
  
  # fix the seed so that result could be reproducible
  set.seed(1) 
  
  # each data has its own id(1 to 5) to process k fold cross validation 
  data$id <- sample(1:k, nrow(data), replace = TRUE)
  list    <- 1:k
  
  # data frame reset
  prediction_ada <- testset_copy_ada <- data.frame()
  
  #function for k fold
  for(i in 1:k){
    # divide the whole dataset into train and testset
    trainset     <- subset(data, id %in% list[-i])
    testset      <- subset(data, id %in% c(i))
    
    #run a adaboost model
    model_ada                <- basic.adaboost(trainset[, independent], trainset[, dependent], n_rounds = iteration)
    k.path <<- list()
    # predict
    temp_ada                 <- as.data.frame(predict(model_ada, testset))
    
    # append this iteration's prediction to the end of the prediction data frame
    prediction_ada           <- rbind(prediction_ada, temp_ada)
    
    # append this iteration's test set to the testset copy data frame
    testset_copy_ada         <- rbind(testset_copy_ada, as.data.frame(testset[, dependent]))
    
    # result
    result_ada               <- cbind(prediction_ada, testset_copy_ada[, 1])
    
    # confustion matrix and accuracy
    names(result_ada)        <- c("Actual", "Predicted")
    confusion_matrix_ada     <- table(result_ada$Actual, result_ada$Predicted)
    accuracy_ada             <- sum(diag(confusion_matrix_ada)) / sum(confusion_matrix_ada)
    result_ada               <- list("confusion_matrix_ada " = confusion_matrix_ada, 
                                     "accuracy_ada"          = accuracy_ada)
  }  
  
  # store the accuracy of each model
  acc_ada    <<- result_ada$accuracy_ada         
  
  print((acc_ada))
}


for (i in 1:100){
  k.k <<- 0.01*i
  k.list <- c(1/k.k, 1, k.k)
  k.path <<- list()
  kfold.ada(100)
}
